{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkKK3qqLDdkb",
        "outputId": "3c7d0243-b57b-45dd-ef33-80e3bc27d391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.4/313.4 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade gradio\n",
        "!pip install --quiet openai\n",
        "!pip install --quiet astrapy\n",
        "!pip install --quiet voyageai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B43ZPs-6_zd5",
        "outputId": "709b0515-e611-4476-c27e-9e70e304e57b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to Astra DB: {'status': {'collections': ['StudyBuddy']}}\n"
          ]
        }
      ],
      "source": [
        "#Setting up useful libraries\n",
        "from openai import OpenAI\n",
        "import sys\n",
        "import codecs\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "import voyageai\n",
        "from astrapy.db import AstraDB\n",
        "import gradio as gr\n",
        "\n",
        "vec_db = AstraDB(\n",
        "  token=\"AstraCS:JojuxKENIQUZbHOHsOKzqdxZ:311ac2f4ac1f981f7c69a4bb8566b0bcac48193257fa845da7cff3aabb38395f\",\n",
        "  api_endpoint=\"https://8b4697c6-4dca-40ac-8a07-7f098c39314b-us-east1.apps.astra.datastax.com\",# This has the dataset\n",
        "  namespace = 'student_services')\n",
        "\n",
        "print(f\"Connected to Astra DB: {vec_db.get_collections()}\")\n",
        "\n",
        "vstore = vec_db.create_collection('StudyBuddy', dimension=1536, metric='cosine')\n",
        "\n",
        "vo = voyageai.Client(api_key= '*********************************')# Add your voyage ai api key here\n",
        "\n",
        "# Set the collection name\n",
        "collection = vec_db.collection(collection_name=\"StudyBuddy\")\n",
        "\n",
        "client = OpenAI(\n",
        "      base_url=\"https://api-inference.huggingface.co/v1\",\n",
        "      api_key = '***************************************',# Add your huggingface api key here\n",
        "  )\n",
        "\n",
        "client_open_ai = OpenAI(api_key = '*****************************************')# Add your openai api key here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "HKQxptDrzOI8",
        "outputId": "08b6f042-3019-47e1-b998-3a6a924ad72f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://9fc7bdc37595bb7629.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://9fc7bdc37595bb7629.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "recommendations = None\n",
        "possible_matches = None\n",
        "student_details = None\n",
        "student_name = None\n",
        "\n",
        "\n",
        "with gr.Blocks(\n",
        "    title = 'Expatrio Matching Algorithm'\n",
        ") as demo:\n",
        "    gr.Markdown(\"Answer the following few questions to build your query. Feel free to leave blank or put N/A for questions that you prefer not to answer / do not apply. You can also add any additional preferences or information in the last question.\")\n",
        "    name = gr.Textbox(label=\"What is your name?\")\n",
        "    degree = gr.Textbox(label=\"What degree are you currently seeking (Masters, PhD etc)?\")\n",
        "    major = gr.Textbox(label=\"Could you share your intended major?\")\n",
        "    tuition = gr.Textbox(label=\"Are you working within a preferred tuition budget?\")\n",
        "    style_of_learning = gr.Textbox(label=\"Would you say you learn more towards hands-on, experiential learning where you actively engage in practical activities and projects, or do you feel more comfortable with traditional classroom-based learning, focusing on lectures and discussions?\")\n",
        "    location = gr.Textbox(label=\"Do you have a preference for the location of your program within Germany?\")\n",
        "    top_priority = gr.Textbox(label=\"Among academics, professional opportunities, social environment, and cost of living, which factor holds the greatest importance for you when selecting a university?\")\n",
        "    social_life_importance = gr.Textbox(label=\"On a scale of 1 to 5, to what extent do you value maintaining an active social life and participating in extracurricular activities beyond your academic studies?\")\n",
        "    financial_aid_relevance = gr.Textbox(label=\"Could financial aid or scholarship opportunities play a significant role in your decision to enroll in a university?\")\n",
        "    cohort_size = gr.Textbox(label=\"When it comes to class sizes, do you prefer smaller cohorts for a more personalized learning experience, or do you thrive in larger academic communities with a diverse range of perspectives?\")\n",
        "    inclusion_diversity = gr.Textbox(label=\"To what extent do you value attending a university that places a strong emphasis on inclusion and diversity among its student body and faculty?\")\n",
        "    projects = gr.Textbox(label=\"Are you particular about participating in research projects, internships, or other hands-on learning opportunities during your undergraduate studies?\")\n",
        "    student_clubs = gr.Textbox(label=\"Are you particular on joining any particular student clubs or organizations on campus?\")\n",
        "    reputation = gr.Textbox(label=\"How much importance do you place on the reputation and prestige of a university when deciding where to study?\")\n",
        "    preferences = gr.Textbox(label=\"Do you have any peculiar preferences not listed above?\")\n",
        "    submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
        "    clear_btn = gr.ClearButton(value = \"Reset All\", components=[name, degree, major, tuition, style_of_learning, location, top_priority, social_life_importance, financial_aid_relevance, cohort_size, inclusion_diversity, projects, student_clubs, reputation, preferences])\n",
        "    output= gr.Textbox(label=\"This is the summary of your data. Feel free to adjust any information you'd like to change.\", interactive=True)\n",
        "    all_good_btn = gr.Button(\"This looks good\", variant=\"secondary\")\n",
        "\n",
        "    @submit_btn.click(inputs=[name, degree, major, tuition, style_of_learning, location, top_priority, social_life_importance, financial_aid_relevance, cohort_size, inclusion_diversity, projects, student_clubs, reputation, preferences], outputs=output)\n",
        "\n",
        "    def build_query(name, degree, major, tuition, style_of_learning, location, top_priority, social_life_importance, financial_aid_relevance, cohort_size, inclusion_diversity, reputation, projects, student_clubs, preferences):\n",
        "        query = f\"\"\"\n",
        "        My name is {name}. I am seeking a {degree} course with {major} major. My tuition fee constraints are {tuition},\n",
        "        {style_of_learning} is my preferred style of learning, {location} is my preferred location for the program.\n",
        "        My topmost priority in choosing a program is one with a good {top_priority}. On a scale of 1 to 5, {social_life_importance} is my preference for a program with strong social activities and {financial_aid_relevance} is how important financial aid is to me.\n",
        "        My preferences for the program's cohort size is {cohort_size}, inclusion and diversity is {inclusion_diversity}, reputation is {reputation}.\n",
        "        I would love a program with the opportunity to engage in {projects} and join {student_clubs} student clubs. Finally, {preferences} are additional preferences to keep in mind.\"\"\"\n",
        "\n",
        "        global student_name\n",
        "        student_name = name\n",
        "\n",
        "        chat_completion = client.chat.completions.create(\n",
        "          model = 'google/gemma-7b-it', #gpt-4\n",
        "          messages = [\n",
        "              {\n",
        "                  \"role\": \"user\",\n",
        "                  \"content\": f\"\"\"\n",
        "                  Correct the provided query {query} for a user who is looking for a program that matches their needs to make it more coherent.\n",
        "                  Provide only the rewritten query starting with \"My name is\" without additional remarks. If some spaces in the query are blank, leave them blank. Do not hallucinate information to fill them in.\n",
        "                  \"\"\"\n",
        "              }\n",
        "          ],\n",
        "          max_tokens=5000\n",
        "        )\n",
        "\n",
        "        global student_details\n",
        "        student_details = chat_completion.choices[0].message.content\n",
        "\n",
        "        return student_details\n",
        "\n",
        "    output.input(outputs=output)\n",
        "\n",
        "    #print(output)\n",
        "    recommendationsTextbox = gr.Textbox(label=\"Based on your query, here are some recommendations for you. Feel free to ask further questions about the recommended programs in the chatbot below.\")\n",
        "\n",
        "    @all_good_btn.click(inputs=[output], outputs=[recommendationsTextbox])\n",
        "\n",
        "    def get_chat_matches(query):\n",
        "        def get_matching_embeddings(query):\n",
        "            embeddings = vo.embed(query, model='voyage-large-2', input_type='query').embeddings[0]\n",
        "            max_records = 3\n",
        "\n",
        "            # Retrieve all documents based on the vector search\n",
        "            result = collection.vector_find(\n",
        "                vector=embeddings,\n",
        "                limit=max_records\n",
        "            )\n",
        "\n",
        "            response = []\n",
        "\n",
        "            for i in range(len(result)):\n",
        "                response.append(f\"{result[i]['content']}\")\n",
        "                #print(result[i]['content'])\n",
        "                #print('\\n')\n",
        "\n",
        "            return response\n",
        "\n",
        "        global possible_matches\n",
        "        possible_matches = get_matching_embeddings(query)\n",
        "\n",
        "        chat_completion = client.chat.completions.create(\n",
        "                model = 'google/gemma-7b-it', #gpt-4\n",
        "                messages = [\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f\"\"\" You are a course adviser for a student looking for a course in Germany.\n",
        "                        Given the student's query {student_details} and possible responses {possible_matches}\n",
        "                        Start by saying 'Hey {student_name}, based on your interests and preferences, I found the following courses'.\n",
        "                        To determine the best fit, prioritize the response that aligns most with {degree} and then {major}.\n",
        "                        You may also draw on external, accurate knowledge about the suggested university/program to determine if it is a good fit\n",
        "                        Present the best fit and highlight why each is a good fit for their query and which ways it doesn't fit the query perfectly\n",
        "                        \"\"\"\n",
        "                    }\n",
        "                ],\n",
        "                max_tokens=5000\n",
        "            )\n",
        "        #Then present each of the possible responses to the the student highlighting why each is a good fit for their query\n",
        "        global recommendations\n",
        "\n",
        "        recommendations = chat_completion.choices[0].message.content\n",
        "        return recommendations\n",
        "\n",
        "    def predict(message, history):\n",
        "      if not recommendations:\n",
        "        yield 'Please fill out the form above first'\n",
        "\n",
        "      else:\n",
        "        history_openai_format = []\n",
        "\n",
        "        for human, assistant in history:\n",
        "          history_openai_format.append({\"role\": \"user\", \"content\": human })\n",
        "          history_openai_format.append({\"role\": \"assistant\", \"content\": assistant})\n",
        "\n",
        "        history_openai_format.append({\"role\": \"user\", \"content\": message})\n",
        "        system_message = {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"\"\"\n",
        "                def drawUponKnowledgeFromTrainingData(message):\n",
        "                  You are a friendly expert on German universities and programs. Use your knowledge to respond to message.\n",
        "                  return answerToMessage\n",
        "\n",
        "                def giveResponseToMessage(student_details, possible_matches, recommendations, message):\n",
        "                  '''\n",
        "                  Given a message, respond to the student in an engaging way\n",
        "\n",
        "                  Input\n",
        "                  student_details: A summary of the students preferences in searching for a German program\n",
        "                  possible_matches: Programs they have been matched with\n",
        "                  recommendations: A summary of the possible matches and how they fit the students details\n",
        "                  message: Student's additional question.\n",
        "\n",
        "                  Output:\n",
        "                  answerToMessage: Your response to the student's additional question.\n",
        "                  '''\n",
        "\n",
        "                  if answer to message not in student_details or possible_matches or recommendations:\n",
        "                    respond from your general knowledge\n",
        "\n",
        "                  else:\n",
        "                    check (student_details and possible_matches and recommendations):\n",
        "                      return answerToMessage.\n",
        "\n",
        "                  def giveResponseToMessage({student_details, possible_matches, recommendations, message})\n",
        "                \"\"\"}\n",
        "\n",
        "        #Engage conversationally with the student's message, {message} and draw upon your extensive knowledge of German universities and programs to help them where necessary.\n",
        "        #Where necessary to respond, you may draw upon the background information about the student: {student_details}. They have been matched with the following possible programs: {possible_matches} and provided the following recommendations: {recommendations}.\n",
        "        response = client_open_ai.chat.completions.create(model='gpt-3.5-turbo',\n",
        "        messages= [system_message, {'role': 'user', 'content': message}],\n",
        "        stream=False)\n",
        "\n",
        "        #print(response.choices[0].message.content)\n",
        "        yield response.choices[0].message.content\n",
        "        '''\n",
        "        responseString = \"\"\n",
        "        for chunk in response:\n",
        "            if chunk.choices[0].delta.content is not None:\n",
        "                  responseString = responseString + chunk.choices[0].delta.content\n",
        "        print(responseString)\n",
        "\n",
        "        partial_message = \"\"\n",
        "        for chunk in response:\n",
        "            if chunk.choices[0].delta.content is not None:\n",
        "                  partial_message = partial_message + chunk.choices[0].delta.content\n",
        "                  yield partial_message\n",
        "        '''\n",
        "\n",
        "    gr.ChatInterface(predict)\n",
        "\n",
        "demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
